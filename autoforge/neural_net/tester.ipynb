{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "735414eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2   0.8  -0.5   1.  ]\n",
      " [ 0.5  -0.91  0.26 -0.5 ]\n",
      " [-0.26 -0.27  0.17  0.87]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]])\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71c093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39eaf5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tes = np.random.rand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed71592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_tes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d66ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.24277071, 0.60349031, 0.98350394, 0.10918128],\n",
       "        [0.16325947, 0.65026198, 0.02012676, 0.67590188],\n",
       "        [0.36087923, 0.24319907, 0.0882304 , 0.74343206]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w_tes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2275afee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2bd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "467a7be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c554b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff15997e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32721627",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.random.randn(60, n_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc9f376e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010d6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1faabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = (32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e990a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = [n_features] + list(hidden_layer_size) + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ac63bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 32, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b217d76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5078da8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ea68c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(layer_size[0], layer_size[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "332af4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = []\n",
    "bias = []\n",
    "\n",
    "for i in range(len(layer_size) - 1):\n",
    "    w = np.random.randn(layer_size[i], layer_size[i+1])\n",
    "\n",
    "    b = np.zeros((1, layer_size[i+1]))\n",
    "\n",
    "\n",
    "    weight.append(w)\n",
    "    bias.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab3d2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e51ded99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 1.1855\n",
      "Epoch 100 | Loss 0.3542\n",
      "Epoch 200 | Loss 0.3318\n",
      "Epoch 300 | Loss 0.3228\n",
      "Epoch 400 | Loss 0.3174\n",
      "Epoch 500 | Loss 0.3138\n",
      "Epoch 600 | Loss 0.3110\n",
      "Epoch 700 | Loss 0.3090\n",
      "Epoch 800 | Loss 0.3073\n",
      "Epoch 900 | Loss 0.3060\n",
      "Accuracy: 0.855\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "# from autoforge.base import BaseEstimator, require_fit\n",
    "from activations import sigmoid\n",
    "from activations import Relu\n",
    "\n",
    "\n",
    "class MLPClassifier:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        BaseEstimator (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_layer_sizes, activation, learning_rate, max_iter, random_state\n",
    "    ):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.lr = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def backward(self, X, y, y_pred):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        dz = (y_pred - y) / m\n",
    "        \n",
    "        for i in reversed(range(len(self.weight))):\n",
    "            dw = self.As[i].T @ dz\n",
    "            db = dz.sum(axis=0, keepdims=True)\n",
    "\n",
    "            if i > 0:\n",
    "                da = dz @ self.weight[i].T\n",
    "                dz = da * self.activation.backward(self.Zs[i - 1])\n",
    "\n",
    "            self.weight[i] -= self.lr * dw\n",
    "            self.bias[i] -= self.lr * db\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        self.activation = Relu()\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        self.weight = []\n",
    "        self.bias = []\n",
    "\n",
    "        self.layer_size = [n_features] + list(self.hidden_layer_sizes) + [1]\n",
    "\n",
    "        for i in range(len(self.layer_size) - 1):\n",
    "            w = np.random.randn(self.layer_size[i], self.layer_size[i + 1]) * np.sqrt(2 / self.layer_size[i])\n",
    "\n",
    "            b = np.zeros((1, self.layer_size[i + 1]))\n",
    "\n",
    "            self.weight.append(w)\n",
    "            self.bias.append(b)\n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(self.max_iter):\n",
    "            A = X\n",
    "            self.Zs = []\n",
    "            self.As = [X]\n",
    "\n",
    "            for i in range(len(self.weight)):\n",
    "                self.z = A @ self.weight[i] + self.bias[i]\n",
    "                self.Zs.append(self.z)\n",
    "\n",
    "                if i == len(self.weight) - 1:\n",
    "                    A = sigmoid.forward(self.z)\n",
    "\n",
    "                else:\n",
    "                    A = self.activation.forward(self.z)\n",
    "\n",
    "                self.As.append(A)\n",
    "\n",
    "            y_hat = A\n",
    "\n",
    "            loss = -np.mean(\n",
    "                y * np.log(y_hat + 1e-8) + (1 - y) * np.log(1 - y_hat + 1e-8)\n",
    "            )\n",
    "            self.backward(X, y, y_hat)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} | Loss {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        A = X\n",
    "\n",
    "        for i in range(len(self.weight)):\n",
    "            Z = A @ self.weight[i] + self.bias[i]\n",
    "\n",
    "            if i == len(self.weight) - 1:\n",
    "                A = sigmoid.forward(Z)\n",
    "\n",
    "            else:\n",
    "                A = self.activation.forward(Z)\n",
    "\n",
    "        return (A > 0.5).astype(int).flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training\n",
    "\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, ),\n",
    "    activation=\"relu\",\n",
    "    learning_rate=0.01,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41b73dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e401e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred.flatten().shape\n",
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7eba9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "# print(f\"Original y shape: {y.shape}\")  # (1000,) - 1D\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# print(f\"y_test shape: {y_test.shape}\")  # (200,) - Still 1D\n",
    "\n",
    "# clf = MLPClassifier()\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print(f\"y_pred shape: {y_pred.shape}\")  # (200,) - 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c16352a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bc81d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfe864ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_list = [10, 20, 30, 40, 50]\n",
    "\n",
    "# counts = 0\n",
    "# for item in reversed(range(len(my_list))):\n",
    "#     print(item)\n",
    "#     counts += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8bb61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26a77a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from numpy.random import rand\n",
    "\n",
    "# # from autoforge.base import BaseEstimator, require_fit\n",
    "# from activations import sigmoid\n",
    "# from activations import Relu\n",
    "\n",
    "\n",
    "# class MLPClassifier:\n",
    "#     \"\"\"_summary_\n",
    "\n",
    "#     Args:\n",
    "#         BaseEstimator (_type_): _description_\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, hidden_layer_sizes, activation, learning_rate, max_iter, random_state\n",
    "#     ):\n",
    "#         self.hidden_layer_sizes = hidden_layer_sizes\n",
    "#         self.activation = activation\n",
    "#         self.lr = learning_rate\n",
    "#         self.max_iter = max_iter\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def backward(self, X, y, y_pred):\n",
    "#         m = X.shape[0]\n",
    "\n",
    "#         # Gradient of loss w.r.t. output (with sigmoid derivative already included)\n",
    "#         dz = (y_pred - y) / m\n",
    "        \n",
    "#         for i in reversed(range(len(self.weight))):\n",
    "#             dw = self.As[i].T @ dz\n",
    "#             db = dz.sum(axis=0, keepdims=True)\n",
    "\n",
    "#             if i > 0:\n",
    "#                 da = dz @ self.weight[i].T\n",
    "#                 dz = da * self.activation.backward(self.Zs[i - 1])\n",
    "\n",
    "#             self.weight[i] -= self.lr * dw\n",
    "#             self.bias[i] -= self.lr * db\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         np.random.seed(self.random_state)\n",
    "#         self.activation = Relu()\n",
    "\n",
    "#         n_features = X.shape[1]\n",
    "\n",
    "#         self.weight = []\n",
    "#         self.bias = []\n",
    "\n",
    "#         self.layer_size = [n_features] + list(self.hidden_layer_sizes) + [1]\n",
    "\n",
    "#         for i in range(len(self.layer_size) - 1):\n",
    "#             w = np.random.randn(self.layer_size[i], self.layer_size[i + 1]) * np.sqrt(2 / self.layer_size[i])\n",
    "#             b = np.zeros((1, self.layer_size[i + 1]))\n",
    "\n",
    "#             self.weight.append(w)\n",
    "#             self.bias.append(b)\n",
    "\n",
    "#         # Training Loop\n",
    "#         for epoch in range(self.max_iter):\n",
    "#             A = X\n",
    "#             self.Zs = []\n",
    "#             self.As = [X]\n",
    "\n",
    "#             for i in range(len(self.weight)):\n",
    "#                 self.z = A @ self.weight[i] + self.bias[i]\n",
    "#                 self.Zs.append(self.z)\n",
    "\n",
    "#                 if i == len(self.weight) - 1:\n",
    "#                     A = sigmoid.forward(self.z)\n",
    "#                 else:\n",
    "#                     A = self.activation.forward(self.z)\n",
    "\n",
    "#                 self.As.append(A)\n",
    "\n",
    "#             y_hat = A\n",
    "\n",
    "#             loss = -np.mean(\n",
    "#                 y * np.log(y_hat + 1e-8) + (1 - y) * np.log(1 - y_hat + 1e-8)\n",
    "#             )\n",
    "#             self.backward(X, y, y_hat)\n",
    "\n",
    "#             if epoch % 100 == 0:\n",
    "#                 print(f\"Epoch {epoch} | Loss {loss:.4f}\")\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         X = np.asarray(X)\n",
    "\n",
    "#         A = X\n",
    "\n",
    "#         for i in range(len(self.weight)):\n",
    "#             Z = A @ self.weight[i] + self.bias[i]\n",
    "\n",
    "#             if i == len(self.weight) - 1:\n",
    "#                 A = sigmoid.forward(Z)\n",
    "#             else:\n",
    "#                 A = self.activation.forward(Z)\n",
    "\n",
    "#         return (A > 0.5).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# X, y = make_classification(\n",
    "#     n_samples=1000,\n",
    "#     n_features=2,\n",
    "#     n_informative=2,\n",
    "#     n_redundant=0,\n",
    "#     n_classes=2,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# y = y.reshape(-1, 1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # IMPORTANT: Scale the features!\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Training\n",
    "# model = MLPClassifier(\n",
    "#     hidden_layer_sizes=(32, ),\n",
    "#     activation=\"relu\",\n",
    "#     learning_rate=0.1,  # Increased learning rate\n",
    "#     max_iter=1000,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# accuracy = np.mean(y_pred.flatten() == y_test.flatten())\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5579ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dog:\n",
    "    species = \"canine\"\n",
    "\n",
    "    @classmethod\n",
    "    def info(cls):\n",
    "        print(cls.species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "74b2e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "canine\n"
     ]
    }
   ],
   "source": [
    "Dog.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c97f57b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 0.4030\n",
      "Epoch 100 | Loss 0.2238\n",
      "Epoch 200 | Loss 0.2052\n",
      "Epoch 300 | Loss 0.1966\n",
      "Epoch 400 | Loss 0.1938\n",
      "Epoch 500 | Loss 0.1904\n",
      "Epoch 600 | Loss 0.1900\n",
      "Epoch 700 | Loss 0.1891\n",
      "Epoch 800 | Loss 0.1889\n",
      "Epoch 900 | Loss 0.1885\n",
      "Accuracy: 0.935\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from autoforge.base import BaseEstimator, require_fit\n",
    "# from autoforge.neural_net.activations import ACTIVATIONS, sigmoid\n",
    "from activations import ACTIVATIONS, sigmoid\n",
    "\n",
    "\n",
    "class MLPClassifier:\n",
    "    \"\"\"\n",
    "    Multi-layer Perceptron Classifier\n",
    "\n",
    "    A feedforward neural network for binary classification using backpropagation\n",
    "    with binary cross-entropy loss and sigmoid output activation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hidden_layer_sizes : tuple of int, default=(100,)\n",
    "        The number of neurons in each hidden layer.\n",
    "        For example, (64, 32) creates two hidden layers with 64 and 32 neurons.\n",
    "    activation : str or BaseActivation, default='relu'\n",
    "        Activation function for the hidden layers.\n",
    "        Supported strings: 'relu', 'tanh', 'sigmoid', 'leaky_relu', 'elu', 'swish'.\n",
    "    learning_rate : float, default=0.01\n",
    "        Learning rate for gradient descent weight updates.\n",
    "        Controls the step size during optimization.\n",
    "    max_iter : int, default=200\n",
    "        Maximum number of training iterations (epochs).\n",
    "    random_state : int or None, default=None\n",
    "        Random seed for reproducible weight initialization.\n",
    "        Pass an integer for reproducible output across multiple function calls.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_layer_sizes=(100,),\n",
    "        activation=\"relu\",\n",
    "        learning_rate=0.01,\n",
    "        max_iter=200,\n",
    "        random_state=None,\n",
    "        batch_size=32\n",
    "    ):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.lr = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _initialize_weight(self, n_features):\n",
    "        \"\"\"Initialize network weights\"\"\"\n",
    "        self.weight = []\n",
    "        self.bias = []\n",
    "\n",
    "        self.layer_size = [n_features] + list(self.hidden_layer_sizes) + [1]\n",
    "\n",
    "        for i in range(len(self.layer_size) - 1):\n",
    "            w = np.random.randn(self.layer_size[i], self.layer_size[i + 1]) * np.sqrt(\n",
    "                2 / self.layer_size[i]\n",
    "            )\n",
    "\n",
    "            b = np.zeros((1, self.layer_size[i + 1]))\n",
    "\n",
    "            self.weight.append(w)\n",
    "            self.bias.append(b)\n",
    "\n",
    "    def _forward_pass(self, X, predict=False):\n",
    "        \"\"\"Perform forward propagation through the network.\"\"\"\n",
    "        A = X\n",
    "\n",
    "        if not predict:\n",
    "            self.Zs = []\n",
    "            self.As = [X]\n",
    "\n",
    "        for i in range(len(self.weight)):\n",
    "            # Compute pre-activation\n",
    "            self.z = A @ self.weight[i] + self.bias[i]\n",
    "\n",
    "            if not predict:\n",
    "                self.Zs.append(self.z)\n",
    "\n",
    "            # Apply activation function\n",
    "            if i == len(self.weight) - 1:\n",
    "                # Output layer: sigmoid for binary classification\n",
    "                A = sigmoid.forward(self.z)\n",
    "\n",
    "            else:\n",
    "                # Hidden layers: use specified activation\n",
    "                A = self.activation_func.forward(self.z)\n",
    "\n",
    "            if not predict:\n",
    "                self.As.append(A)\n",
    "\n",
    "        return A\n",
    "\n",
    "    def _backward_pass(self, X, y, y_pred):\n",
    "        \"\"\" Perform backpropagation to compute gradients and update weights. \"\"\"\n",
    "        m = X.shape[0]\n",
    "\n",
    "        # Compute output gradient\n",
    "        dz = (y_pred - y) / m\n",
    "\n",
    "        # Backpropagate through layers\n",
    "        for i in reversed(range(len(self.weight))):\n",
    "            # Compute gradient for current layer\n",
    "            dw = self.As[i].T @ dz\n",
    "            db = dz.sum(axis=0, keepdims=True)\n",
    "\n",
    "            # Propagate gradient to previous layer\n",
    "            if i > 0:\n",
    "                da = dz @ self.weight[i].T\n",
    "                dz = da * self.activation_func.backward(self.Zs[i - 1])\n",
    "\n",
    "            # Update weights using optimizer\n",
    "            self.weight[i] -= self.lr * dw\n",
    "            self.bias[i] -= self.lr * db\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the MLP classifier to training data. \"\"\"\n",
    "        y = y.reshape(-1, 1)\n",
    "\n",
    "        self.activation_func = ACTIVATIONS[self.activation]\n",
    "        # print(_activ)\n",
    "\n",
    "        # Initialize components\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Intiaialize weights\n",
    "        self._initialize_weight(n_features)\n",
    "        # Training Loop\n",
    "        for epoch in range(self.max_iter):\n",
    "            idx = np.random.permutation(n_samples)\n",
    "\n",
    "            X_shuffled = X[idx]\n",
    "            y_shuffled = y[idx]\n",
    "\n",
    "            epoch_loss = 0\n",
    "            n_batches = 0\n",
    "\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                \n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "                y_hat = self._forward_pass(X_batch, predict=False)\n",
    "\n",
    "                batch_loss = -np.mean(\n",
    "                    y_batch * np.log(y_hat + 1e-8) + (1 - y_batch) * np.log(1 - y_hat + 1e-8)\n",
    "                )\n",
    "\n",
    "                epoch_loss += batch_loss\n",
    "                n_batches += 1\n",
    "\n",
    "                self._backward_pass(X_batch, y_batch, y_hat)\n",
    "\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} | Loss {avg_loss:.4f}\")\n",
    " \n",
    "        # self._mark_fitted()\n",
    "        return self\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict binary class labels for samples.\"\"\"\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        return (self._predict_proba(X) > 0.5).astype(int).flatten()\n",
    "\n",
    "    def _predict_proba(self, X):\n",
    "        \"\"\" Predict class probabilities for samples.\"\"\"\n",
    "        # Forward pass without storing activations (memory efficient)\n",
    "        A = self._forward_pass(X, predict=True)\n",
    "        return A\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\" Return the mean accuracy on the given test data and labels. \"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        return np.mean(y_pred == y)\n",
    "# End\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "y = y.reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# IMPORTANT: Scale the features!\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(32, ),\n",
    "    activation=\"relu\",\n",
    "    learning_rate=0.1,  # Increased learning rate\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = np.mean(y_pred.flatten() == y_test.flatten())\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
