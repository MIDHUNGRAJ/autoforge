{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "735414eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2   0.8  -0.5   1.  ]\n",
      " [ 0.5  -0.91  0.26 -0.5 ]\n",
      " [-0.26 -0.27  0.17  0.87]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1],\n",
    "                    [0.5, -0.91, 0.26, -0.5],\n",
    "                    [-0.26, -0.27, 0.17, 0.87]])\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c71c093b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39eaf5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tes = np.random.rand(3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eed71592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_tes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d66ac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.9564392 , 0.75196229, 0.96628323, 0.651441  ],\n",
       "        [0.68159071, 0.69593172, 0.91738471, 0.38615949],\n",
       "        [0.40584755, 0.16289921, 0.93830472, 0.6086575 ]])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w_tes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2275afee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(w_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fd2bd341",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "467a7be6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4c554b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff15997e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32721627",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = np.random.randn(60, n_features) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc9f376e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010d6113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1faabb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_size = (32,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e990a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_size = [n_features] + list(hidden_layer_size) + [1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ac63bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 32, 1]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b217d76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_size[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5078da8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(layer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ea68c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randn(layer_size[0], layer_size[1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "332af4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight = []\n",
    "bias = []\n",
    "\n",
    "for i in range(len(layer_size) - 1):\n",
    "    w = np.random.randn(layer_size[i], layer_size[i+1])\n",
    "\n",
    "    b = np.zeros((1, layer_size[i+1]))\n",
    "\n",
    "\n",
    "    weight.append(w)\n",
    "    bias.append(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab3d2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e51ded99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss 0.8780\n",
      "Epoch 100 | Loss 0.3456\n",
      "Epoch 200 | Loss 0.3181\n",
      "Epoch 300 | Loss 0.3096\n",
      "Epoch 400 | Loss 0.3055\n",
      "Epoch 500 | Loss 0.3030\n",
      "Epoch 600 | Loss 0.3013\n",
      "Epoch 700 | Loss 0.2999\n",
      "Epoch 800 | Loss 0.2987\n",
      "Epoch 900 | Loss 0.2978\n",
      "Accuracy: 0.835\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.random import rand\n",
    "\n",
    "# from autoforge.base import BaseEstimator, require_fit\n",
    "from activations import sigmoid\n",
    "from activations import Relu\n",
    "\n",
    "\n",
    "class MLPClassifier:\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        BaseEstimator (_type_): _description_\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hidden_layer_sizes, activation, learning_rate, max_iter, random_state\n",
    "    ):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.lr = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def backward(self, X, y, y_pred):\n",
    "        m = X.shape[0]\n",
    "\n",
    "        dz = (y_pred - y) / m\n",
    "        \n",
    "        for i in reversed(range(len(self.weight))):\n",
    "            dw = self.As[i].T @ dz\n",
    "            db = dz.sum(axis=0, keepdims=True)\n",
    "\n",
    "            if i > 0:\n",
    "                da = dz @ self.weight[i].T\n",
    "                dz = da * self.activation.backward(self.Zs[i - 1])\n",
    "\n",
    "            self.weight[i] -= self.lr * dw\n",
    "            self.bias[i] -= self.lr * db\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        y = y.reshape(-1, 1)\n",
    "        \n",
    "        self.activation = Relu()\n",
    "\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        self.weight = []\n",
    "        self.bias = []\n",
    "\n",
    "        self.layer_size = [n_features] + list(self.hidden_layer_sizes) + [1]\n",
    "\n",
    "        for i in range(len(self.layer_size) - 1):\n",
    "            w = np.random.randn(self.layer_size[i], self.layer_size[i + 1]) * np.sqrt(2 / self.layer_size[i])\n",
    "\n",
    "            b = np.zeros((1, self.layer_size[i + 1]))\n",
    "\n",
    "            self.weight.append(w)\n",
    "            self.bias.append(b)\n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(self.max_iter):\n",
    "            A = X\n",
    "            self.Zs = []\n",
    "            self.As = [X]\n",
    "\n",
    "            for i in range(len(self.weight)):\n",
    "                self.z = A @ self.weight[i] + self.bias[i]\n",
    "                self.Zs.append(self.z)\n",
    "\n",
    "                if i == len(self.weight) - 1:\n",
    "                    A = sigmoid.forward(self.z)\n",
    "\n",
    "                else:\n",
    "                    A = self.activation.forward(self.z)\n",
    "\n",
    "                self.As.append(A)\n",
    "\n",
    "            y_hat = A\n",
    "\n",
    "            loss = -np.mean(\n",
    "                y * np.log(y_hat + 1e-8) + (1 - y) * np.log(1 - y_hat + 1e-8)\n",
    "            )\n",
    "            self.backward(X, y, y_hat)\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                print(f\"Epoch {epoch} | Loss {loss:.4f}\")\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X)\n",
    "\n",
    "        A = X\n",
    "\n",
    "        for i in range(len(self.weight)):\n",
    "            Z = A @ self.weight[i] + self.bias[i]\n",
    "\n",
    "            if i == len(self.weight) - 1:\n",
    "                A = sigmoid.forward(Z)\n",
    "\n",
    "            else:\n",
    "                A = self.activation.forward(Z)\n",
    "\n",
    "        return (A > 0.5).astype(int).flatten()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=2,\n",
    "    n_informative=2,\n",
    "    n_redundant=0,\n",
    "    n_classes=2,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Training\n",
    "\n",
    "model = MLPClassifier(\n",
    "    hidden_layer_sizes=(64, ),\n",
    "    activation=\"relu\",\n",
    "    learning_rate=0.01,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# End\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "41b73dee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e401e9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred.flatten().shape\n",
    "# y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b7eba9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "# print(f\"Original y shape: {y.shape}\")  # (1000,) - 1D\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "# print(f\"y_test shape: {y_test.shape}\")  # (200,) - Still 1D\n",
    "\n",
    "# clf = MLPClassifier()\n",
    "# clf.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = clf.predict(X_test)\n",
    "# print(f\"y_pred shape: {y_pred.shape}\")  # (200,) - 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c16352a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bc81d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dfe864ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_list = [10, 20, 30, 40, 50]\n",
    "\n",
    "# counts = 0\n",
    "# for item in reversed(range(len(my_list))):\n",
    "#     print(item)\n",
    "#     counts += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8bb61c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "26a77a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from numpy.random import rand\n",
    "\n",
    "# # from autoforge.base import BaseEstimator, require_fit\n",
    "# from activations import sigmoid\n",
    "# from activations import Relu\n",
    "\n",
    "\n",
    "# class MLPClassifier:\n",
    "#     \"\"\"_summary_\n",
    "\n",
    "#     Args:\n",
    "#         BaseEstimator (_type_): _description_\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(\n",
    "#         self, hidden_layer_sizes, activation, learning_rate, max_iter, random_state\n",
    "#     ):\n",
    "#         self.hidden_layer_sizes = hidden_layer_sizes\n",
    "#         self.activation = activation\n",
    "#         self.lr = learning_rate\n",
    "#         self.max_iter = max_iter\n",
    "#         self.random_state = random_state\n",
    "\n",
    "#     def backward(self, X, y, y_pred):\n",
    "#         m = X.shape[0]\n",
    "\n",
    "#         # Gradient of loss w.r.t. output (with sigmoid derivative already included)\n",
    "#         dz = (y_pred - y) / m\n",
    "        \n",
    "#         for i in reversed(range(len(self.weight))):\n",
    "#             dw = self.As[i].T @ dz\n",
    "#             db = dz.sum(axis=0, keepdims=True)\n",
    "\n",
    "#             if i > 0:\n",
    "#                 da = dz @ self.weight[i].T\n",
    "#                 dz = da * self.activation.backward(self.Zs[i - 1])\n",
    "\n",
    "#             self.weight[i] -= self.lr * dw\n",
    "#             self.bias[i] -= self.lr * db\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         np.random.seed(self.random_state)\n",
    "#         self.activation = Relu()\n",
    "\n",
    "#         n_features = X.shape[1]\n",
    "\n",
    "#         self.weight = []\n",
    "#         self.bias = []\n",
    "\n",
    "#         self.layer_size = [n_features] + list(self.hidden_layer_sizes) + [1]\n",
    "\n",
    "#         for i in range(len(self.layer_size) - 1):\n",
    "#             w = np.random.randn(self.layer_size[i], self.layer_size[i + 1]) * np.sqrt(2 / self.layer_size[i])\n",
    "#             b = np.zeros((1, self.layer_size[i + 1]))\n",
    "\n",
    "#             self.weight.append(w)\n",
    "#             self.bias.append(b)\n",
    "\n",
    "#         # Training Loop\n",
    "#         for epoch in range(self.max_iter):\n",
    "#             A = X\n",
    "#             self.Zs = []\n",
    "#             self.As = [X]\n",
    "\n",
    "#             for i in range(len(self.weight)):\n",
    "#                 self.z = A @ self.weight[i] + self.bias[i]\n",
    "#                 self.Zs.append(self.z)\n",
    "\n",
    "#                 if i == len(self.weight) - 1:\n",
    "#                     A = sigmoid.forward(self.z)\n",
    "#                 else:\n",
    "#                     A = self.activation.forward(self.z)\n",
    "\n",
    "#                 self.As.append(A)\n",
    "\n",
    "#             y_hat = A\n",
    "\n",
    "#             loss = -np.mean(\n",
    "#                 y * np.log(y_hat + 1e-8) + (1 - y) * np.log(1 - y_hat + 1e-8)\n",
    "#             )\n",
    "#             self.backward(X, y, y_hat)\n",
    "\n",
    "#             if epoch % 100 == 0:\n",
    "#                 print(f\"Epoch {epoch} | Loss {loss:.4f}\")\n",
    "\n",
    "#     def predict(self, X):\n",
    "#         X = np.asarray(X)\n",
    "\n",
    "#         A = X\n",
    "\n",
    "#         for i in range(len(self.weight)):\n",
    "#             Z = A @ self.weight[i] + self.bias[i]\n",
    "\n",
    "#             if i == len(self.weight) - 1:\n",
    "#                 A = sigmoid.forward(Z)\n",
    "#             else:\n",
    "#                 A = self.activation.forward(Z)\n",
    "\n",
    "#         return (A > 0.5).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# X, y = make_classification(\n",
    "#     n_samples=1000,\n",
    "#     n_features=2,\n",
    "#     n_informative=2,\n",
    "#     n_redundant=0,\n",
    "#     n_classes=2,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# y = y.reshape(-1, 1)\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # IMPORTANT: Scale the features!\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "# # Training\n",
    "# model = MLPClassifier(\n",
    "#     hidden_layer_sizes=(32, ),\n",
    "#     activation=\"relu\",\n",
    "#     learning_rate=0.1,  # Increased learning rate\n",
    "#     max_iter=1000,\n",
    "#     random_state=42,\n",
    "# )\n",
    "\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# accuracy = np.mean(y_pred.flatten() == y_test.flatten())\n",
    "# print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5579ae54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "engine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
